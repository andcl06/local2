# app.py
# Streamlit ê¸°ë°˜ HEART Insight AI ì›¹ ì†”ë£¨ì…˜ì˜ ë©”ì¸ íŒŒì¼ (ëª¨ë“  ê¸°ëŠ¥ í†µí•©)

# -----------------------------------------------------
# 1. ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ (íŒŒì¼ ìµœìƒë‹¨)
# -----------------------------------------------------
import streamlit as st
import os
from dotenv import load_dotenv
from loguru import logger
import pandas as pd
import time
import uuid

# 'modules' í´ë”ì— ìˆëŠ” ì»¤ìŠ¤í…€ ëª¨ë“ˆë“¤ì„ ì„í¬íŠ¸í•©ë‹ˆë‹¤.
from modules import ai_interface
from modules import data_collector
from modules import trend_analyzer
from modules import document_processor

# --- LangChain RAG ê¸°ëŠ¥ì— í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ---
import tiktoken
import tempfile
from langchain.chains import ConversationalRetrievalChain
from langchain.chat_models import ChatOpenAI
from langchain.document_loaders import PyPDFLoader, Docx2txtLoader, UnstructuredPowerPointLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.memory import ConversationBufferMemory
from langchain.vectorstores import FAISS
from langchain.callbacks import get_openai_callback
from langchain.memory import StreamlitChatMessageHistory

# -----------------------------------------------------
# 2. ë©”ì¸ Streamlit ì• í”Œë¦¬ì¼€ì´ì…˜ í•¨ìˆ˜ ì •ì˜ (def main():)
# -----------------------------------------------------
def main():
    # -----------------
    # 2-1. í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
    # -----------------
    load_dotenv()
    POTENS_API_KEY = os.getenv("POTENS_API_KEY")
    
    # -----------------
    # 2-2. í˜ì´ì§€ ê¸°ë³¸ ì„¤ì • (ì²« Streamlit ëª…ë ¹ì–´ì—¬ì•¼ í•¨)
    # -----------------
    st.set_page_config(
        page_title="í˜„ëŒ€í•´ìƒ HEART Insight AI",
        page_icon="ğŸš—",
        layout="wide",
        initial_sidebar_state="expanded"
    )

    # -----------------
    # 2-3. Streamlit ì„¸ì…˜ ìƒíƒœ(Session State) ì´ˆê¸°í™”
    # -----------------
    if "messages" not in st.session_state: st.session_state["messages"] = []
    if "api_ready" not in st.session_state: st.session_state["api_ready"] = False
    if "data_collected" not in st.session_state: st.session_state["data_collected"] = False
    if "collected_data" not in st.session_state: st.session_state["collected_data"] = []
    if "topic_analysis_result" not in st.session_state: st.session_state["topic_analysis_result"] = None
    if "rag_conversation" not in st.session_state: st.session_state["rag_conversation"] = None
    if "rag_processed" not in st.session_state: st.session_state["rag_processed"] = False

    # -----------------
    # 2-4. UI êµ¬ì„±: ë©”ì¸ í™”ë©´
    # -----------------
    st.title("ğŸš— í˜„ëŒ€í•´ìƒ HEART Insight AI")
    st.subheader("ë¯¸ë˜ ëª¨ë¹Œë¦¬í‹° íŠ¸ë Œë“œ ë¶„ì„ ë° ë³´í—˜ ì‹œì‚¬ì  ë„ì¶œ ì†”ë£¨ì…˜")
    st.markdown("---")
    st.markdown(
        """
        **HEART Insight AI**ëŠ” ê¸‰ë³€í•˜ëŠ” ë¯¸ë˜ ëª¨ë¹Œë¦¬í‹° í™˜ê²½ì˜ íŠ¸ë Œë“œë¥¼ ì‹¬ì¸µ ë¶„ì„í•˜ê³ ,
        ì´ë¥¼ í˜„ëŒ€í•´ìƒì˜ ë³´í—˜ ìƒí’ˆ ê°œë°œ ë° ë¦¬ìŠ¤í¬ í‰ê°€ì— í•„ìš”í•œ í•µì‹¬ ì‹œì‚¬ì ê³¼ ê¸°íšŒ ìš”ì¸ìœ¼ë¡œ ë„ì¶œí•˜ëŠ” AI ê¸°ë°˜ ì†”ë£¨ì…˜ì…ë‹ˆë‹¤.
        """
    )

# 5. AI íŠ¸ë Œë“œ ë¶„ì„ ëŒ€ì‹œë³´ë“œ
# -----------------
    st.header("ğŸ“ˆ AI íŠ¸ë Œë“œ ë¶„ì„ ëŒ€ì‹œë³´ë“œ")
    st.info("ì´ê³³ì— ë‰´ìŠ¤, ë³´ê³ ì„œ, íŠ¹í—ˆ ë“±ì—ì„œ ìˆ˜ì§‘ëœ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•œ íŠ¸ë Œë“œ ì˜ˆì¸¡ ê·¸ë˜í”„, í‚¤ì›Œë“œ ë„¤íŠ¸ì›Œí¬, í† í”½ ëª¨ë¸ë§ ê²°ê³¼ ë“± ì¸í„°ë™í‹°ë¸Œ ì‹œê°í™”ê°€ êµ¬í˜„ë  ì˜ˆì •ì…ë‹ˆë‹¤.")

    # --- ìˆ˜ì§‘ëœ ë°ì´í„° ë° ë¶„ì„ ê²°ê³¼ê°€ ìˆì„ ë•Œë§Œ ëŒ€ì‹œë³´ë“œ ë‚´ìš© í‘œì‹œ ---
    if st.session_state.data_collected:
        st.markdown("### ìˆ˜ì§‘ëœ ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°")
        # ìˆ˜ì§‘ëœ ë°ì´í„° ì¤‘ 10ê°œë§Œ ë¯¸ë¦¬ë³´ê¸°
        st.dataframe(pd.DataFrame(st.session_state.collected_data)[:10], use_container_width=True)
        st.markdown("---")

        if st.session_state.topic_analysis_result and st.session_state.topic_analysis_result['fig_html']:
            st.markdown("### ğŸ“Š í† í”½ ëª¨ë¸ë§ ì‹œê°í™”")
            # Plotly ê·¸ë˜í”„ë¥¼ HTML ì»´í¬ë„ŒíŠ¸ë¡œ í‘œì‹œ
            st.components.v1.html(st.session_state.topic_analysis_result['fig_html'], height=600)
            st.markdown("---")

            st.markdown("### ğŸ“ ì£¼ìš” íŠ¸ë Œë“œ (í† í”½) ìš”ì•½")
            # í† í”½ ì •ë³´ë¥¼ ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ í‘œì‹œ
            topic_info_df = pd.DataFrame(st.session_state.topic_analysis_result['topic_info'])
            topic_info_df.index.name = 'Topic ID'
            st.dataframe(topic_info_df[['Count', 'Name', 'Representation']], use_container_width=True)
            st.success("âœ… íŠ¸ë Œë“œ ë¶„ì„ ê²°ê³¼ê°€ ëŒ€ì‹œë³´ë“œì— ë°˜ì˜ë˜ì—ˆìŠµë‹ˆë‹¤!")
        else:
            st.warning("í† í”½ ëª¨ë¸ë§ ê²°ê³¼ë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ë¡œê·¸ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")

    st.markdown("---")

    # -----------------
    # 2-5. ëŒ€í™”í˜• AI ì±—ë´‡ (Potens.dev API & RAG í†µí•©)
    # -----------------
    st.header("ğŸ’¬ íŠ¸ë Œë“œ ë¶„ì„ Q&A ì±—ë´‡")
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    user_query = st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
    if user_query:
        st.session_state.messages.append({"role": "user", "content": user_query})
        with st.chat_message("user"):
            st.markdown(user_query)
        with st.chat_message("assistant"):
            with st.spinner("ë‹µë³€ì„ ìƒì„± ì¤‘ì…ë‹ˆë‹¤..."):
                response = ""
                if st.session_state.rag_processed and st.session_state.rag_conversation:
                    chain = st.session_state.rag_conversation
                    try:
                        result = chain({"question": user_query})
                        response = result['answer']
                    except Exception as e:
                        response = f"ë¬¸ì„œ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"
                        logger.error(f"RAG chatbot error: {e}")
                elif st.session_state.api_ready:
                    response = ai_interface.call_potens_api(
                        prompt_message=user_query, api_key=POTENS_API_KEY, history=st.session_state.messages
                    )
                else:
                    response = "âš ï¸ ì±—ë´‡ ê¸°ëŠ¥ì´ ë¹„í™œì„±í™”ë˜ì—ˆìŠµë‹ˆë‹¤. ì‚¬ì´ë“œë°”ì—ì„œ ê¸°ëŠ¥ì„ í™œì„±í™”í•´ì£¼ì„¸ìš”."
                st.markdown(response)
                st.session_state.messages.append({"role": "assistant", "content": response})
    else:
        if not st.session_state.api_ready and not st.session_state.rag_processed:
            st.warning("âš ï¸ ì±—ë´‡ ê¸°ëŠ¥ì´ ë¹„í™œì„±í™”ë˜ì—ˆìŠµë‹ˆë‹¤. ì‚¬ì´ë“œë°”ì—ì„œ ê¸°ëŠ¥ì„ ì„ íƒí•˜ê³  ì¤€ë¹„í•´ì£¼ì„¸ìš”.")
            st.markdown("`AI ì±—ë´‡ ì¤€ë¹„` ë˜ëŠ” `Process Documents` ë²„íŠ¼ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”.")

    # -----------------
    # 2-6. ì‚¬ì´ë“œë°” UI (ê¸°ëŠ¥ ì œì–´ ë²„íŠ¼)
    # -----------------
    with st.sidebar:
        st.header("í”„ë¡œì íŠ¸ ì •ë³´")
        st.markdown("**í”„ë¡œì íŠ¸ëª…:** HEART Insight AI")
        st.markdown("**ê°œë°œ:** ë©”ì´ì»¤ìŠ¤ë©")
        st.markdown("---")
        st.header("ê¸°ëŠ¥ ì œì–´")

        st.subheader("ğŸ“„ ë¬¸ì„œ ê¸°ë°˜ Q&A ì±—ë´‡")
        uploaded_files = st.file_uploader("Upload your file", type=['pdf', 'docx', 'pptx'], accept_multiple_files=True)
        process_docs_button = st.button("Process Documents", key="process_docs")
        if process_docs_button:
            if not POTENS_API_KEY:
                st.error("âš ï¸ Potens.dev API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. `.env` íŒŒì¼ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
            elif not uploaded_files:
                st.error("âš ï¸ ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")
            else:
                with st.spinner("ë¬¸ì„œ ì²˜ë¦¬ ì¤‘..."):
                    try:
                        os.environ["OPENAI_API_BASE"] = "https://ai.potens.ai/api/chat"
                        files_text = document_processor.get_text(uploaded_files)
                        if not files_text:
                            st.error("âš ï¸ ì—…ë¡œë“œëœ íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
                        else:
                            vectorstore = document_processor.get_vectorstore(document_processor.get_text_chunks(files_text))
                            st.session_state.rag_conversation = document_processor.get_conversation_chain(vectorstore, POTENS_API_KEY)
                            if st.session_state.rag_conversation:
                                st.session_state.rag_processed = True
                                st.success("âœ… ë¬¸ì„œ ì²˜ë¦¬ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ì´ì œ ë¬¸ì„œì— ëŒ€í•´ ì§ˆë¬¸í•´ë³´ì„¸ìš”!")
                    except Exception as e:
                        st.error(f"âŒ ë¬¸ì„œ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
                        st.session_state.rag_processed = False
#íŠ¸ë Œë“œ ìˆ˜ì§‘ ë¶„ì„ ë²„íŠ¼
        st.markdown("---")
        st.header("íŠ¸ë Œë“œ ë°ì´í„° ìˆ˜ì§‘ ë° ë¶„ì„")
        if st.button("íŠ¸ë Œë“œ ë°ì´í„° ìˆ˜ì§‘ ë° ë¶„ì„ ì‹œì‘", help="ë‰´ìŠ¤ ê¸°ì‚¬ë¥¼ í¬ë¡¤ë§í•˜ê³  AI ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤."):
            if not st.session_state.data_collected:
                # API í˜¸ì¶œì„ ìœ„í•´ í‚¤ì›Œë“œë¥¼ ì˜ì–´ë¡œ ë³€ê²½í•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤.
                keywords = ["electric vehicle battery", "self-driving car insurance", "UAM market", "PBV Hyundai", "MaaS service"]
                collected_articles = []
                with st.spinner("ë‰´ìŠ¤ ê¸°ì‚¬ ë°ì´í„° ìˆ˜ì§‘ ì¤‘..."):
                    for keyword in keywords:
                        # scrape_google_news_api í•¨ìˆ˜ í˜¸ì¶œë¡œ ë³€ê²½!
                        articles = data_collector.scrape_google_news_api(keyword, num_results=5) # num_results=5ë¡œ 5ê°œì”© ìš”ì²­
                        collected_articles.extend(articles)
                        st.info(f"'{keyword}' ê´€ë ¨ ê¸°ì‚¬ {len(articles)}ê°œ ìˆ˜ì§‘ ì™„ë£Œ.")
                
                if not collected_articles:
                    st.warning("âš ï¸ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. í…ŒìŠ¤íŠ¸ìš© ë”ë¯¸ ë°ì´í„°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
                    collected_articles = [{"title": "ì „ê¸°ì°¨ ë°°í„°ë¦¬ ë¦¬ìŠ¤í¬", "content": "ë‚´ìš©", "source": "Dummy Data", "keywords": "ì „ê¸°ì°¨"}, {"title": "ììœ¨ì£¼í–‰", "content": "ë‚´ìš©", "source": "Dummy Data", "keywords": "ììœ¨ì£¼í–‰"}] # Simplified dummy data
                if collected_articles:
                    st.session_state.collected_data = collected_articles
                    st.session_state.data_collected = True
                    st.success(f"âœ… ì´ {len(st.session_state.collected_data)}ê°œì˜ ê¸°ì‚¬ ë°ì´í„°ë¡œ ë¶„ì„ì„ ì‹œì‘í•©ë‹ˆë‹¤!")
                    with st.spinner("ìˆ˜ì§‘ëœ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ AI íŠ¸ë Œë“œ ë¶„ì„ ì¤‘..."):
                        analysis_result = trend_analyzer.perform_topic_modeling(st.session_state.collected_data)
                        st.session_state.topic_analysis_result = analysis_result
                    if st.session_state.topic_analysis_result and st.session_state.topic_analysis_result['topics']:
                        st.success("âœ… íŠ¸ë Œë“œ ë¶„ì„ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ëŒ€ì‹œë³´ë“œë¥¼ í™•ì¸í•˜ì„¸ìš”!")
                    else:
                        st.error("âŒ AI ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë¡œê·¸ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
                else:
                    st.error("ë°ì´í„° ìˆ˜ì§‘ê³¼ í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œì— ëª¨ë‘ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
                    st.session_state.data_collected = False
                    st.session_state.topic_analysis_result = None
            else:
                st.info("ë°ì´í„° ìˆ˜ì§‘ ë° ë¶„ì„ì´ ì´ë¯¸ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ì•±ì„ ìƒˆë¡œê³ ì¹¨í•˜ì—¬ ë‹¤ì‹œ ì‹œì‘í•˜ì„¸ìš”.")
# ìš”ê¸°ê¹Œì§€
#aiì±—ë´‡í™œì„±í™”
        st.markdown("---")
        st.subheader("ğŸ’¬ AI íŠ¸ë Œë“œ ì±—ë´‡")
        if st.button("AI ì±—ë´‡ ì¤€ë¹„", key="activate_chatbot"):
            if POTENS_API_KEY:
                st.session_state.api_ready = True
                st.success("ğŸ‰ Potens.dev API ì¤€ë¹„ ì™„ë£Œ! ì´ì œ ì±—ë´‡ì— ì§ˆë¬¸í•´ë³´ì„¸ìš”.")
                st.experimental_rerun()
            else:
                st.error("API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. `.env` íŒŒì¼ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
                st.session_state.api_ready = False

        st.markdown("---")
#aiì±—ë´‡ ë

        st.markdown("---")
        st.header("ëŒ€í™” ì´ˆê¸°í™”")
        if st.button("ì „ì²´ ëŒ€í™” ì´ˆê¸°í™”", help="ëª¨ë“  ëŒ€í™” ê¸°ë¡ê³¼ ì²˜ë¦¬ëœ ë¬¸ì„œë¥¼ ì‚­ì œí•©ë‹ˆë‹¤."):
            st.session_state.messages = []
            st.session_state.rag_processed = False
            st.session_state.rag_conversation = None
            st.session_state.data_collected = False
            st.session_state.collected_data = []
            st.session_state.topic_analysis_result = None
            st.experimental_rerun()

# -----------------------------------------------------
# 3. RAG ê¸°ëŠ¥ Helper í•¨ìˆ˜ë“¤ (def main() í•¨ìˆ˜ ì •ì˜ ì•„ë˜ì— ìœ„ì¹˜)
# -----------------------------------------------------
def tiktoken_len(text):
    tokenizer = tiktoken.get_encoding("cl100k_base")
    return len(tokenizer.encode(text))

def get_text(docs):
    doc_list = []
    with tempfile.TemporaryDirectory() as tmp_dir:
        logger.info(f"Using temporary directory: {tmp_dir}")
        for doc in docs:
            ext = os.path.splitext(doc.name)[1].lower()
            temp_filename = os.path.join(tmp_dir, f"{uuid.uuid4()}{ext}")
            try:
                with open(temp_filename, "wb") as file:
                    file.write(doc.getvalue())
                logger.info(f"Saved {doc.name} to temporary path: {temp_filename}")
            except Exception as e:
                logger.error(f"Error saving file {doc.name} to temp: {e}", exc_info=True)
                continue
            try:
                if ext == '.pdf': loader = PyPDFLoader(temp_filename)
                elif ext == '.docx': loader = Docx2txtLoader(temp_filename)
                elif ext == '.pptx': loader = UnstructuredPowerPointLoader(temp_filename)
                else:
                    logger.warning(f"Unsupported file type: {ext}")
                    continue
                documents = loader.load()
                doc_list.extend(documents)
                logger.info(f"Loaded {len(documents)} documents from {doc.name}")
            except Exception as e:
                logger.error(f"Error loading document from {doc.name}: {e}", exc_info=True)
                continue
    return doc_list

def get_text_chunks(documents):
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=900, chunk_overlap=100, length_function=tiktoken_len)
    chunks = text_splitter.split_documents(documents)
    logger.info(f"Documents split into {len(chunks)} chunks.")
    return chunks

def get_vectorstore(text_chunks):
    try:
        embeddings = HuggingFaceEmbeddings(model_name="jhgan/ko-sroberta-multitask", model_kwargs={'device': 'cpu'}, encode_kwargs={'normalize_embeddings': True})
        vectordb = FAISS.from_documents(text_chunks, embeddings)
        logger.info("Vectorstore created using FAISS.")
        return vectordb
    except Exception as e:
        logger.error(f"Failed to create vectorstore: {e}", exc_info=True)
        return None

def get_conversation_chain(vectorstore, api_key):
    try:
        llm = ChatOpenAI(api_key=api_key, model_name='claude-3.7-sonnet', temperature=0, openai_api_base="https://potens.ai/")
        logger.info(f"ChatOpenAI model initialized with base_url: {llm.openai_api_base}")
        return ConversationalRetrievalChain.from_llm(
            llm=llm, chain_type="stuff", retriever=vectorstore.as_retriever(search_type='mmr', verbose=True),
            memory=ConversationBufferMemory(memory_key='chat_history', return_messages=True, output_key='answer'),
            get_chat_history=lambda h: h, return_source_documents=True, verbose=True
        )
    except Exception as e:
        logger.error(f"Failed to create conversation chain: {e}", exc_info=True)
        return None

# -----------------------------------------------------
# 4. ì•± ì‹¤í–‰ ì§„ì…ì  (íŒŒì¼ ê°€ì¥ ë§ˆì§€ë§‰ì— ìœ„ì¹˜)
# -----------------------------------------------------
if __name__ == '__main__':
    main()