# main_app.py
# ì´ íŒŒì¼ì€ 'ë³´í—˜íŠ¹ì•½ê°œë°œ ì†”ë£¨ì…˜'ì˜ ë©”ì¸ ì• í”Œë¦¬ì¼€ì´ì…˜ìœ¼ë¡œ,
# ë¡œê·¸ì¸, ëœë”© í˜ì´ì§€, ìµœì‹  íŠ¸ë Œë“œ ë¶„ì„, ë¬¸ì„œ ë¶„ì„ ê¸°ëŠ¥ì„ í†µí•©í•©ë‹ˆë‹¤.

import streamlit as st
import pandas as pd
import streamlit.components.v1 as components
import os
from dotenv import load_dotenv
from loguru import logger
import requests
import tiktoken # document.pyì—ì„œ ì‚¬ìš©
from langchain.document_loaders import PyPDFLoader, Docx2txtLoader, UnstructuredPowerPointLoader, TextLoader # <-- TextLoader ì„í¬íŠ¸ í™•ì¸
from langchain.text_splitter import RecursiveCharacterTextSplitter # document.pyì—ì„œ ì‚¬ìš©
from langchain.embeddings import HuggingFaceEmbeddings # document.pyì—ì„œ ì‚¬ìš©
from langchain.vectorstores import FAISS # document.pyì—ì„œ ì‚¬ìš©
from langchain.memory import StreamlitChatMessageHistory # document.pyì—ì„œ ì‚¬ìš©

# modules ë””ë ‰í† ë¦¬ ë‚´ì˜ ì»¤ìŠ¤í…€ ëª¨ë“ˆ ì„í¬íŠ¸
# ì´ íŒŒì¼ë“¤ì´ ì‹¤ì œ ë””ë ‰í† ë¦¬ì— ì¡´ì¬í•´ì•¼ í•©ë‹ˆë‹¤.
from modules.data_collector import scrape_google_news_api
from modules.trend_detector import get_articles_from_db, detect_trending_keywords, get_articles_by_keywords
from modules.report_generator import create_single_page_report

# --- í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ (ë¡œì»¬ì—ì„œ .env íŒŒì¼ ì‚¬ìš©) ---
load_dotenv()

# --- Potens API í˜¸ì¶œ í•¨ìˆ˜ (ë‘ ê¸°ëŠ¥ì—ì„œ ê³µí†µìœ¼ë¡œ ì‚¬ìš©) ---
def call_potens_api(prompt, api_key):
    """
    Potens.dev AI APIë¥¼ í˜¸ì¶œí•˜ì—¬ í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    """
    url = "https://ai.potens.ai/api/chat"
    headers = {
        "Authorization": f"Bearer {api_key}",
        "Content-Type": "application/json"
    }
    data = {"prompt": prompt}

    try:
        response = requests.post(url, headers=headers, json=data, timeout=300) # íƒ€ì„ì•„ì›ƒ ì¶”ê°€
        response.raise_for_status()
        result = response.json()

        if isinstance(result, str):
            return result, []
        elif isinstance(result, dict):
            for key in ["response", "answer", "message", "content", "text", "data"]:
                if key in result:
                    return result[key], result.get("source_documents", [])
            return str(result), result.get("source_documents", [])
        else:
            return str(result), []
    except requests.exceptions.RequestException as e:
        logger.error(f"Potens API ìš”ì²­ ì˜¤ë¥˜: {e}")
        return f"ERROR: Potens API ìš”ì²­ ì¤‘ ì˜¤ë¥˜ ë°œìƒ - {str(e)}", []
    except Exception as e:
        logger.error(f"Potens API ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
        return f"ERROR: Potens API ì‘ë‹µ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ - {str(e)}", []

# --- ë¬¸ì„œ ë¶„ì„ê¸° ê´€ë ¨ í—¬í¼ í•¨ìˆ˜ë“¤ (document.pyì—ì„œ ê°€ì ¸ì˜´) ---
def tiktoken_len(text):
    """í…ìŠ¤íŠ¸ì˜ í† í° ê¸¸ì´ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤."""
    tokenizer = tiktoken.get_encoding("cl100k_base")
    return len(tokenizer.encode(text))

def get_text(uploaded_files):
    """ì—…ë¡œë“œëœ ë¬¸ì„œì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    all_docs = []
    for doc in uploaded_files:
        file_name = doc.name
        # Streamlit í™˜ê²½ì—ì„œëŠ” íŒŒì¼ì„ ì§ì ‘ ì €ì¥í•´ì•¼ ë¡œë”ê°€ ì ‘ê·¼ ê°€ëŠ¥
        with open(file_name, "wb") as f:
            f.write(doc.getvalue())
            logger.info(f"Uploaded: {file_name}")

        if file_name.endswith('.pdf'):
            loader = PyPDFLoader(file_name)
        elif file_name.endswith('.docx'):
            loader = Docx2txtLoader(file_name)
        elif file_name.endswith('.pptx'):
            loader = UnstructuredPowerPointLoader(file_name)
        elif file_name.endswith('.txt'): # <-- TXT íŒŒì¼ ì²˜ë¦¬ ë¡œì§ ì¶”ê°€
            loader = TextLoader(file_name, encoding="utf-8")
        else:
            logger.warning(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤: {file_name}")
            continue

        all_docs.extend(loader.load_and_split())
    return all_docs

def get_text_chunks(texts):
    """í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ ë‹¨ìœ„ë¡œ ë¶„í• í•©ë‹ˆë‹¤."""
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=900,
        chunk_overlap=100,
        length_function=tiktoken_len
    )
    return splitter.split_documents(texts)

def get_vectorstore(chunks):
    """í…ìŠ¤íŠ¸ ì²­í¬ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
    embeddings = HuggingFaceEmbeddings(
        model_name="jhgan/ko-sroberta-multitask",
        model_kwargs={'device': 'cpu'},
        encode_kwargs={'normalize_embeddings': True}
    )
    return FAISS.from_documents(chunks, embeddings)

# --- ì‚¬ìš©ì DB (login.pyì—ì„œ ê°€ì ¸ì˜´) ---
USER_DB = {
    "admin": "1234",
    "guest": "abcd",
    "user" : "qwer",
    "localai" : "asdf"
}

# --- ë¡œê·¸ì¸ í˜ì´ì§€ (login.pyì—ì„œ ê°€ì ¸ì˜´) ---
def login_page():
    """ì‚¬ìš©ì ë¡œê·¸ì¸ ì¸í„°í˜ì´ìŠ¤ë¥¼ ë Œë”ë§í•©ë‹ˆë‹¤."""
    st.title("ğŸ” ë¡œê·¸ì¸ í˜ì´ì§€")

    with st.form("login_form"):
        username = st.text_input("ì•„ì´ë””")
        password = st.text_input("ë¹„ë°€ë²ˆí˜¸", type="password")
        submitted = st.form_submit_button("ë¡œê·¸ì¸")

        if submitted:
            if username in USER_DB and USER_DB[username] == password:
                st.session_state.logged_in = True
                st.session_state.username = username
                st.session_state.page = "landing"
                st.success("âœ… ë¡œê·¸ì¸ ì„±ê³µ!")
                st.experimental_rerun()
            else:
                st.error("âŒ ì•„ì´ë”” ë˜ëŠ” ë¹„ë°€ë²ˆí˜¸ê°€ ì˜ëª»ë˜ì—ˆìŠµë‹ˆë‹¤.")

# --- ëœë”© í˜ì´ì§€ (login.pyì—ì„œ ê°€ì ¸ì˜´) ---
def landing_page():
    """ë¡œê·¸ì¸ í›„ ì‚¬ìš©ìê°€ ê¸°ëŠ¥ì„ ì„ íƒí•˜ëŠ” ëœë”© í˜ì´ì§€ë¥¼ ë Œë”ë§í•©ë‹ˆë‹¤."""
    st.title(f"ğŸ‘‹ {st.session_state.username}ë‹˜, í™˜ì˜í•©ë‹ˆë‹¤!")
    st.subheader("ì›í•˜ëŠ” ê¸°ëŠ¥ì„ ì„ íƒí•˜ì„¸ìš”:")

    col1, col2 = st.columns(2)

    with col1:
        if st.button("ğŸ“ˆ ìµœì‹  íŠ¸ë Œë“œ ë¶„ì„ ì…ì¥"):
            st.session_state.page = "trend"

    with col2:
        if st.button("ğŸ“„ ë¬¸ì„œ ë¶„ì„ ì…ì¥"):
            st.session_state.page = "document"

    st.markdown("---")
    if st.button("ğŸšª ë¡œê·¸ì•„ì›ƒ"):
        st.session_state.logged_in = False
        st.session_state.username = ""
        st.session_state.page = "login"
        st.success("ë¡œê·¸ì•„ì›ƒ ë˜ì—ˆìŠµë‹ˆë‹¤.")
        st.experimental_rerun()

# --- ìµœì‹  íŠ¸ë Œë“œ ë¶„ì„ í˜ì´ì§€ (app.py ë‚´ìš© í†µí•©) ---
def trend_analysis_page():
    """
    ìµœì‹  ë‰´ìŠ¤ ê¸°ë°˜ íŠ¸ë Œë“œ ë¶„ì„ ë° ë³´ê³ ì„œ ìƒì„±ì„ ìˆ˜í–‰í•˜ëŠ” í˜ì´ì§€ì…ë‹ˆë‹¤.
    ê¸°ì¡´ app.pyì˜ ë‚´ìš©ì„ í†µí•©í•©ë‹ˆë‹¤.
    """
    st.title("ğŸ“ˆ HEART Insight AI: ìµœì‹  ëª¨ë¹Œë¦¬í‹° íŠ¸ë Œë“œ ë¶„ì„ ë° ë³´ê³ ì„œ")
    st.markdown("---")

    # ë©”ì¸ìœ¼ë¡œ ëŒì•„ê°€ê¸° ë²„íŠ¼
    if st.button("â¬…ï¸ ë©”ì¸ìœ¼ë¡œ"):
        st.session_state.page = "landing"
        st.experimental_rerun()
    st.markdown("---") # ë²„íŠ¼ ì•„ë˜ êµ¬ë¶„ì„  ì¶”ê°€

    # --- API í‚¤ ë¡œë“œ ---
    POTENS_API_KEY = os.getenv("POTENS_API_KEY")
    if not POTENS_API_KEY:
        st.error("Potens.dev API í‚¤(POTENS_API_KEY)ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env íŒŒì¼ì„ í™•ì¸í•˜ê±°ë‚˜ Streamlit Secretsì— ì„¤ì •í•´ì£¼ì„¸ìš”.")
        st.stop()

    # --- 1. í‚¤ì›Œë“œ ì…ë ¥ ê¸°ëŠ¥ ---
    default_keywords = 'electric vehicle battery, self-driving car insurance, UAM market, PBV Hyundai, MaaS service'
    keywords_input = st.text_input(
        "ë¶„ì„í•  í‚¤ì›Œë“œë¥¼ ì½¤ë§ˆ(,)ë¡œ êµ¬ë¶„í•˜ì—¬ ì…ë ¥í•˜ì„¸ìš”:",
        default_keywords,
        key="keywords_input_box"
    )

    # --- ë¶„ì„ ì‹¤í–‰ ë²„íŠ¼ ---
    if st.button("ìµœì‹  ë‰´ìŠ¤ ê¸°ë°˜ íŠ¸ë Œë“œ ë¶„ì„ ì‹¤í–‰í•˜ê¸°", key="run_analysis_button"):
        if keywords_input:
            keywords = [kw.strip() for kw in keywords_input.split(',') if kw.strip()]
            st.info(f"ì…ë ¥ëœ í‚¤ì›Œë“œ: {keywords}")
        else:
            st.warning("ë¶„ì„í•  í‚¤ì›Œë“œë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
            st.stop()

        with st.spinner("ìµœì‹  ë‰´ìŠ¤ë¥¼ ìˆ˜ì§‘í•˜ê³  íŠ¸ë Œë“œ ê°ì§€ë¥¼ ìˆ˜í–‰ ì¤‘ì…ë‹ˆë‹¤... (ì•½ 1~2ë¶„ ì†Œìš”)"):
            # 1. ë°ì´í„° ìˆ˜ì§‘ (data_collector.py)
            articles_fetched_for_session = []
            for keyword in keywords:
                articles_fetched_for_session.extend(scrape_google_news_api(keyword, num_results=5)) # ê° í‚¤ì›Œë“œë‹¹ 5ê°œ ê¸°ì‚¬
            
            st.session_state['all_articles'] = articles_fetched_for_session

            if st.session_state['all_articles']:
                st.success(f"ì´ {len(st.session_state['all_articles'])}ê°œì˜ ë‰´ìŠ¤ ê¸°ì‚¬ë¥¼ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤.")
                
                # 2. íŠ¸ë Œë“œ ê°ì§€ (trend_detector.py)
                # DBì—ì„œ ìµœê·¼ ê¸°ì‚¬ë¥¼ ë¡œë“œí•˜ì—¬ íŠ¸ë Œë“œ ê°ì§€ (scrape_google_news_apiì—ì„œ ì´ë¯¸ DBì— ì €ì¥í–ˆìœ¼ë¯€ë¡œ)
                recent_articles_from_db = get_articles_from_db(days_ago=30) # ìµœê·¼ 30ì¼ ë°ì´í„° ì‚¬ìš©
                if recent_articles_from_db:
                    detected_trends = detect_trending_keywords(recent_articles_from_db, lookback_days=7, threshold_percent_increase=50.0)
                    st.session_state['detected_trends'] = detected_trends
                    st.success("íŠ¸ë Œë“œ ê°ì§€ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
                else:
                    st.warning("ë°ì´í„°ë² ì´ìŠ¤ì— ìµœê·¼ ê¸°ì‚¬ê°€ ì—†ì–´ íŠ¸ë Œë“œ ê°ì§€ë¥¼ ìˆ˜í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                    st.session_state['detected_trends'] = []
            else:
                st.error("ë‰´ìŠ¤ ê¸°ì‚¬ ìˆ˜ì§‘ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. API í‚¤ë‚˜ ë„¤íŠ¸ì›Œí¬ ì„¤ì •ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
                st.session_state['all_articles'] = []
                st.session_state['detected_trends'] = []
                st.session_state['selected_trend_keyword'] = None # ì´ˆê¸°í™”
                st.session_state['selected_trend_articles'] = [] # ì´ˆê¸°í™”
                st.session_state['report_path'] = None # ì´ˆê¸°í™”

    # --- 2. ê°ì§€ëœ íŠ¸ë Œë“œ í‘œì‹œ ë° ì„ íƒ ê¸°ëŠ¥ ---
    if st.session_state['detected_trends']:
        st.subheader("ğŸ” ê°ì§€ëœ ìµœì‹  íŠ¸ë Œë“œ")
        
        trend_options = [
            f"{trend['keyword']} (ì–¸ê¸‰ëŸ‰: {trend['current_mentions']}íšŒ, ì¦ê°€ìœ¨: {trend['percent_increase']}%)" 
            for trend in st.session_state['detected_trends']
        ]
        
        selected_trend_display = st.selectbox(
            "ë³´ê³ ì„œë¥¼ ìƒì„±í•  íŠ¸ë Œë“œë¥¼ ì„ íƒí•˜ì„¸ìš”:",
            options=["--- íŠ¸ë Œë“œ ì„ íƒ ---"] + trend_options,
            key="trend_selector"
        )

        if selected_trend_display != "--- íŠ¸ë Œë“œ ì„ íƒ ---":
            # ì„ íƒëœ íŠ¸ë Œë“œ í‚¤ì›Œë“œ ì¶”ì¶œ
            selected_keyword_index = trend_options.index(selected_trend_display)
            st.session_state['selected_trend_keyword'] = st.session_state['detected_trends'][selected_keyword_index]['keyword']
            selected_trend_reason = f"í‚¤ì›Œë“œ '{st.session_state['selected_trend_keyword']}' ì–¸ê¸‰ëŸ‰ ê¸‰ì¦ (í˜„ì¬ {st.session_state['detected_trends'][selected_keyword_index]['current_mentions']}íšŒ, ì´ì „ {st.session_state['detected_trends'][selected_keyword_index]['previous_mentions']}íšŒ, ì¦ê°€ìœ¨ {st.session_state['detected_trends'][selected_keyword_index]['percent_increase']}%)"
            
            st.info(f"ì„ íƒëœ íŠ¸ë Œë“œ: **{st.session_state['selected_trend_keyword']}**")

            # ì„ íƒëœ íŠ¸ë Œë“œì— ëŒ€í•œ ê´€ë ¨ ê¸°ì‚¬ ë¡œë“œ (trend_detector.py)
            st.session_state['selected_trend_articles'] = get_articles_by_keywords([st.session_state['selected_trend_keyword']], days_ago=14)
            
            if st.session_state['selected_trend_articles']:
                st.subheader("ğŸ“„ ê´€ë ¨ ë‰´ìŠ¤ ê¸°ì‚¬ (ë³´ê³ ì„œ ìƒì„±ì— í™œìš©)")
                for i, article in enumerate(st.session_state['selected_trend_articles'][:5]): # ìƒìœ„ 5ê°œë§Œ í‘œì‹œ
                    st.markdown(f"**{i+1}. [{article['source']}] {article['title']}**")
                    st.markdown(f"ë°œí–‰ì¼: {pd.to_datetime(article['publish_date']).strftime('%Y.%m.%d')} | [ë§í¬]({article['url']})")
                    st.markdown(f"_{article['content'][:150]}..._")
                    st.markdown("---")
                
                # --- 3. ë³´ê³ ì„œ ìƒì„± ë²„íŠ¼ ---
                if st.button(f"'{st.session_state['selected_trend_keyword']}' íŠ¸ë Œë“œ ë³´ê³ ì„œ ìƒì„±í•˜ê¸°", key="generate_report_button"):
                    with st.spinner("AI ë¶„ì„ ë³´ê³ ì„œë¥¼ ìƒì„± ì¤‘ì…ë‹ˆë‹¤... (ìµœëŒ€ 5ë¶„ ì†Œìš”)"):
                        # ë³´ê³ ì„œ ìƒì„± (report_generator.py)
                        # report_generator.py ë‚´ë¶€ì—ì„œ call_potens_apië¥¼ í˜¸ì¶œí•˜ë¯€ë¡œ, ì—¬ê¸°ì„œëŠ” api_keyë§Œ ì „ë‹¬
                        report_path = create_single_page_report(
                            trend_name=f"'{st.session_state['selected_trend_keyword']}' ê´€ë ¨ íŠ¸ë Œë“œ",
                            trend_detection_reason=selected_trend_reason,
                            related_articles=st.session_state['selected_trend_articles'],
                            api_key=POTENS_API_KEY, # API í‚¤ ì „ë‹¬
                            max_articles_for_ai_summary=3,
                            delay_between_ai_calls=20
                        )
                        st.session_state['report_path'] = report_path
                        st.success("ë³´ê³ ì„œ ìƒì„±ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
                        
                        if st.session_state['report_path'] and os.path.exists(st.session_state['report_path']):
                            # TXT íŒŒì¼ë¡œ ë‹¤ìš´ë¡œë“œ
                            with open(st.session_state['report_path'], 'rb') as f:
                                st.download_button(
                                    label="ë³´ê³ ì„œ ë‹¤ìš´ë¡œë“œ (.txt)",
                                    data=f.read(),
                                    file_name=os.path.basename(st.session_state['report_path']),
                                    mime="text/plain",
                                    key="download_report_button"
                                )
                        else:
                            st.error("ë³´ê³ ì„œ íŒŒì¼ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ë¡œê·¸ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
            else:
                st.warning("ì„ íƒëœ íŠ¸ë Œë“œì— ëŒ€í•œ ê´€ë ¨ ê¸°ì‚¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    else:
        st.info("ì•„ì§ ê°ì§€ëœ íŠ¸ë Œë“œê°€ ì—†ìŠµë‹ˆë‹¤. ìœ„ì— í‚¤ì›Œë“œë¥¼ ì…ë ¥í•˜ê³  'íŠ¸ë Œë“œ ë¶„ì„ ì‹¤í–‰í•˜ê¸°' ë²„íŠ¼ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”.")


# --- ë¬¸ì„œ ë¶„ì„ í˜ì´ì§€ (document.py ë‚´ìš© í†µí•©) ---
def document_analysis_page():
    """
    ë¬¸ì„œ ê¸°ë°˜ QA ì±—ë´‡ ê¸°ëŠ¥ì„ ì œê³µí•˜ëŠ” í˜ì´ì§€ì…ë‹ˆë‹¤.
    ê¸°ì¡´ document.pyì˜ ë‚´ìš©ì„ í†µí•©í•©ë‹ˆë‹¤.
    """
    st.title("ğŸ“„ _Private Data :red[QA Chat]_")

    # ë©”ì¸ìœ¼ë¡œ ëŒì•„ê°€ê¸° ë²„íŠ¼
    if st.button("â¬…ï¸ ë©”ì¸ìœ¼ë¡œ"):
        st.session_state.page = "landing"
        st.experimental_rerun()
    st.markdown("---") # ë²„íŠ¼ ì•„ë˜ êµ¬ë¶„ì„  ì¶”ê°€

    if "vectordb" not in st.session_state:
        st.session_state.vectordb = None

    with st.sidebar:
        # st.file_uploaderì˜ typeì— 'txt' ì¶”ê°€
        uploaded_files = st.file_uploader("ğŸ“ ë¬¸ì„œ ì—…ë¡œë“œ", type=['pdf', 'docx', 'pptx', 'txt'], accept_multiple_files=True) # <-- 'txt' ì¶”ê°€
        # API í‚¤ëŠ” ì „ì—­ì—ì„œ ë¡œë“œë˜ë¯€ë¡œ, ì—¬ê¸°ì„œëŠ” st.text_inputìœ¼ë¡œ ë‹¤ì‹œ ë°›ì§€ ì•Šê³  ì „ì—­ ë³€ìˆ˜ ì‚¬ìš©
        doc_api_key = os.getenv("POTENS_API_KEY") # Potens API í‚¤ ì¬ì‚¬ìš©
        if not doc_api_key:
            st.warning("Potens API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env íŒŒì¼ì„ í™•ì¸í•˜ê±°ë‚˜ Streamlit Secretsì— ì„¤ì •í•´ì£¼ì„¸ìš”.")
            st.stop()

        process = st.button("ğŸ“š ë¬¸ì„œ ì²˜ë¦¬")

    if process:
        if not uploaded_files:
            st.warning("ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")
            st.stop()

        with st.spinner("ë¬¸ì„œë¥¼ ì²˜ë¦¬ ì¤‘ì…ë‹ˆë‹¤..."):
            docs = get_text(uploaded_files)
            chunks = get_text_chunks(docs)
            vectordb = get_vectorstore(chunks)
            st.session_state.vectordb = vectordb
            st.session_state.docs = docs # 'docs' ì„¸ì…˜ ìƒíƒœ ì¶”ê°€ (íŠ¹ì•½ ìƒì„±ì—ì„œ ì‚¬ìš©)
            st.success("âœ… ë¬¸ì„œ ë¶„ì„ ì™„ë£Œ! ì§ˆë¬¸ì„ ì…ë ¥í•´ë³´ì„¸ìš”.")

    if 'messages' not in st.session_state:
        st.session_state.messages = [{
            "role": "assistant",
            "content": "ì•ˆë…•í•˜ì„¸ìš”! ë¬¸ì„œ ê¸°ë°˜ ì§ˆë¬¸ì„ í•´ë³´ì„¸ìš”."
        }]

    for msg in st.session_state.messages:
        with st.chat_message(msg["role"]):
            st.markdown(msg["content"])

    history = StreamlitChatMessageHistory(key="chat_messages") # StreamlitChatMessageHistory ì´ˆê¸°í™”

    if query := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”."):
        st.session_state.messages.append({"role": "user", "content": query})

        with st.chat_message("user"):
            st.markdown(query)

        with st.chat_message("assistant"):
            if not st.session_state.vectordb:
                st.warning("ë¨¼ì € ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•˜ê³  ì²˜ë¦¬í•´ì•¼ í•©ë‹ˆë‹¤.")
                st.stop()

            with st.spinner("ë‹µë³€ ìƒì„± ì¤‘..."):
                retriever = st.session_state.vectordb.as_retriever(search_type="similarity", k=3)
                docs = retriever.get_relevant_documents(query)

                context = "\n\n".join([doc.page_content for doc in docs])
                final_prompt = f"""ë‹¤ìŒ ë¬¸ì„œë¥¼ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ë‹µí•˜ì„¸ìš”.

[ë¬¸ì„œ ë‚´ìš©]:
{context}

[ì§ˆë¬¸]:
{query}

[ë‹µë³€]:
"""
                # call_potens_api í•¨ìˆ˜ëŠ” ì „ì—­ì— ì •ì˜ëœ ê²ƒì„ ì‚¬ìš©
                answer, _ = call_potens_api(final_prompt, doc_api_key) # doc_api_key ì‚¬ìš©

                st.markdown(answer)
                with st.expander("ğŸ“„ ì°¸ê³  ë¬¸ì„œ"):
                    for doc_ref in docs: # 'doc' ë³€ìˆ˜ëª… ì¶©ëŒ í”¼í•˜ê¸° ìœ„í•´ 'doc_ref' ì‚¬ìš©
                        st.markdown(f"**ì¶œì²˜**: {doc_ref.metadata.get('source', 'ì•Œ ìˆ˜ ì—†ìŒ')}")
                        st.markdown(doc_ref.page_content)

                st.session_state.messages.append({"role": "assistant", "content": answer})

    # --- íŠ¹ì•½ ìƒì„± ê¸°ëŠ¥ (document.pyì—ì„œ ê°€ì ¸ì˜´) ---
    st.subheader("ğŸ“‘ ë³´í—˜ íŠ¹ì•½ ìƒì„±ê¸°")

    # API í‚¤ëŠ” ì´ë¯¸ ì „ì—­ì—ì„œ ë¡œë“œë¨
    # if not doc_api_key: # ì´ë¯¸ ìœ„ì—ì„œ í™•ì¸
    #     st.warning("ë¨¼ì € API í‚¤ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
    #     st.stop()

    if "docs" not in st.session_state: # get_textì—ì„œ ì €ì¥í•œ docs ì‚¬ìš©
        st.warning("ë¬¸ì„œë¥¼ ë¨¼ì € ì—…ë¡œë“œí•˜ê³  ì²˜ë¦¬í•´ì£¼ì„¸ìš”.")
        st.stop()

    generate_special_contract = st.button("âœ¨ íŠ¹ì•½ ìƒì„± ì‹œì‘") # ë²„íŠ¼ ì¶”ê°€

    if generate_special_contract:
        with st.spinner("íŠ¹ì•½ ìƒì„± ì¤‘..."):
            all_text = "\n\n".join([doc.page_content for doc in st.session_state.docs])
            prompt = f"""
ë‹¤ìŒì€ ë³´í—˜ ì•½ê´€ì˜ ë‚´ìš©ì…ë‹ˆë‹¤. ì´ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ê³ ê° ë§ì¶¤í˜• 'íŠ¹ì•½'ì„ 3ê°œ ì œì•ˆí•´ì£¼ì„¸ìš”.
ê° íŠ¹ì•½ì€ ì œëª©ê³¼ ì„¤ëª…ì„ í¬í•¨í•´ì•¼ í•˜ë©°, ì‹¤ì œ ì•½ê´€ì²˜ëŸ¼ ì‘ì„±í•´ì£¼ì„¸ìš”.

[ë³´í—˜ ì•½ê´€]:
{all_text}

[ê²°ê³¼]:
"""
            answer, _ = call_potens_api(prompt, doc_api_key) # doc_api_key ì‚¬ìš©
            st.markdown("### âœ… ìƒì„±ëœ íŠ¹ì•½")
            st.markdown(answer)


# --- ë©”ì¸ ì• í”Œë¦¬ì¼€ì´ì…˜ ë¼ìš°íŒ… ---
def main_app():
    """
    ì• í”Œë¦¬ì¼€ì´ì…˜ì˜ ë©”ì¸ ì§„ì…ì ì…ë‹ˆë‹¤.
    ë¡œê·¸ì¸ ìƒíƒœì— ë”°ë¼ í˜ì´ì§€ë¥¼ ë¼ìš°íŒ…í•©ë‹ˆë‹¤.
    """
    # ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™” (ëª¨ë“  í˜ì´ì§€ì—ì„œ ê³µí†µìœ¼ë¡œ ì‚¬ìš©ë  ë³€ìˆ˜ë“¤)
    if "logged_in" not in st.session_state:
        st.session_state.logged_in = False
    if "username" not in st.session_state:
        st.session_state.username = ""
    if "page" not in st.session_state:
        st.session_state.page = "login"
    
    # app.pyì—ì„œ ì‚¬ìš©ë˜ëŠ” ì„¸ì…˜ ìƒíƒœ ë³€ìˆ˜ ì´ˆê¸°í™”
    if 'all_articles' not in st.session_state:
        st.session_state['all_articles'] = []
    if 'detected_trends' not in st.session_state:
        st.session_state['detected_trends'] = []
    if 'selected_trend_keyword' not in st.session_state:
        st.session_state['selected_trend_keyword'] = None
    if 'selected_trend_articles' not in st.session_state:
        st.session_state['selected_trend_articles'] = []
    if 'report_path' not in st.session_state:
        st.session_state['report_path'] = None
    
    # document.pyì—ì„œ ì‚¬ìš©ë˜ëŠ” ì„¸ì…˜ ìƒíƒœ ë³€ìˆ˜ ì´ˆê¸°í™”
    # document_analysis_page í•¨ìˆ˜ ë‚´ì—ì„œ ì´ˆê¸°í™”ë˜ë¯€ë¡œ ì—¬ê¸°ì„œëŠ” ì œê±°
    # if "vectordb" not in st.session_state:
    #     st.session_state.vectordb = None
    # if 'messages' not in st.session_state:
    #     st.session_state.messages = [{
    #         "role": "assistant",
    #         "content": "ì•ˆë…•í•˜ì„¸ìš”! ë¬¸ì„œ ê¸°ë°˜ ì§ˆë¬¸ì„ í•´ë³´ì„¸ìš”."
    #     }]


    # ë¼ìš°íŒ… ë¡œì§
    if not st.session_state.logged_in:
        login_page()
    else:
        if st.session_state.page == "landing":
            landing_page()
        elif st.session_state.page == "trend":
            trend_analysis_page()
        elif st.session_state.page == "document":
            document_analysis_page()
        else:
            st.session_state.page = "login" # ê¸°ë³¸ê°’ (ë¡œê·¸ì¸ ìƒíƒœì¸ë° í˜ì´ì§€ê°€ ì´ìƒí•˜ë©´ ë¡œê·¸ì¸ìœ¼ë¡œ)


if __name__ == "__main__":
    main_app()

